---
title: "Nicolai_ex6"
author: "Andrea Nicolai"
date: "12/5/2020"
output:
  pdf_document: default 
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Exercise 1

The number of particles emitted by a radioactive source during a fixed interval of time ($\Delta$t = 10 s) follows a Poisson distribution on the parameter $\mu$. The number of particles observed during consecutive time intervals is: 4, 1, 3, 1 and 3.
```{r}
observations <- c(4,1,3,1,3)

delta.mu <- 0.01
mu <- seq(0, 6, by = delta.mu)

median <- function(mu, vector.prob) {
  index <- 1
  for (x in mu) {
    integral <- delta.mu*sum(vector.prob[1:index])
    if (integral > 0.50) break
    index <- index + 1
  }
  return (x)
}

```

a) Suppose a uniform prior distribution for the parameter $\mu$

Determine and draw the posterior distribution for $\mu$, given the data

Evaluate mean, median and variance, both analytically and numerically in R
```{r}
intervals <- length(observations)
tot_observations <- sum(observations)

alpha.post.unif  <- tot_observations + 1
lambda.post.unif <- intervals

post.uniform <- function(x) {
  return (dgamma(x, shape = alpha.post.unif , rate = lambda.post.unif ))
}


mean.analytical.u <- alpha.post.unif/lambda.post.unif
var.analytical.u  <- alpha.post.unif/(lambda.post.unif*lambda.post.unif)
median.analytical.u  <- qgamma(0.50, shape = alpha.post.unif , rate = lambda.post.unif )

mean.computat.u   <- delta.mu*sum(mu*post.uniform(mu))
E2 <- delta.mu*sum(mu*mu*post.uniform(mu))
var.computat.u <- E2 - mean.computat.u*mean.computat.u
median.comput.u <- median(mu, post.uniform(mu))
  
message(sprintf("We obtain a mu of %1.2f for using the analytical formula, while a mu of %1.2f with the explicit computations. They are actually the same value!", mean.analytical.u, mean.computat.u))
message(sprintf("We obtain a std of %1.2f for using the analytical formula, while a std of %1.2f with the explicit computations. They are actually the same value!", var.analytical.u, var.computat.u))
message(sprintf("We obtain a median of %1.2f for using the analytical formula, while a median of %1.2f with the explicit computations. They are the same value!", median.analytical.u, median.comput.u))

plot( mu , post.uniform(mu), type ='l', main = "Posterior - Uniform prior" , col = 'gold3', lwd = 3)
abline (v= mean.analytical.u , col = 'black' , lty=2, xlab = 'mu', ylab = 'probability')
legend("topright", inset = 0.05 , c("Posterior", 'mean'), col = c('gold3', 'black') , lwd = c(3,1), lty = c(1,2))
```

(b) Suppose a Jeffreyâ€™s prior for the parameter $\mu$

Determine and draw the posterior distribution for $\mu$, given the data.
Evaluate mean, median and variance, both analytically and numerically in R.
```{r}
alpha.post.jeffrey  <- tot_observations + 1/2
lambda.post.jeffrey <- intervals

post.jeffrey <- function(x) {
  return (dgamma(x, shape = alpha.post.jeffrey , rate = lambda.post.jeffrey ))
}


mean.analytical.j <- alpha.post.jeffrey/lambda.post.jeffrey
var.analytical.j  <- alpha.post.jeffrey/(lambda.post.jeffrey*lambda.post.jeffrey)
median.analytical.j  <- qgamma(0.50, shape = alpha.post.jeffrey , rate = lambda.post.jeffrey )

mean.computat.j   <- delta.mu*sum(mu*post.jeffrey(mu))
E2 <- delta.mu*sum(mu*mu*post.jeffrey(mu))
var.computat.j <- E2 - mean.computat.j*mean.computat.j
median.comput.j <- median(mu, post.jeffrey(mu))

message(sprintf("We obtain a mu of %1.2f for using the analytical formula, while a mu of %1.2f with the explicit computations. They are actually the same value!", mean.analytical.j, mean.computat.j))
message(sprintf("We obtain a std of %1.2f for using the analytical formula, while a std of %1.2f with the explicit computations. They are actually the same value!", var.analytical.j, var.computat.j))
message(sprintf("We obtain a median of %1.2f for using the analytical formula, while a median of %1.2f with the explicit computations. They are the same value!", median.analytical.j, median.comput.j))

plot( mu , post.jeffrey(mu), type ='l', main = "Posterior - Jeffrey's prior" , col = 'firebrick2', lwd = 3)
abline (v= mean.analytical.j , col = 'black' , lty=2, xlab = 'mu', ylab = 'probability')
legend("topright", inset = 0.05 , c("Posterior", 'mean'), col = c('firebrick2', 'black') , lwd = c(3,1), lty = c(1,2))
```

Let us compare the two priors.
```{r}
plot( mu , post.jeffrey(mu), type ='l', main = "Posteriors" , col = 'firebrick2', lwd = 2, lty = 3)
abline (v= mean.analytical.j , col = 'firebrick2' , lty=2, lwd = 1)

lines( mu, post.uniform(mu), col = 'gold3', lwd = 2, lty = 3)
abline (v= mean.analytical.u , col = 'gold3' , lty=2, lwd = 1)

legend("topright", inset = 0.05 , c("Jeffrey's prior", 'Uniform prior'), col = c('firebrick2', 'gold3') , lwd = c(2,2), lty = c(3,3))
```
Evaluate a 95% credibility interval for the results obtained with both priors. Compare the result with that obtained using a normal approximation for the posterior distribution, with the same mean and standard deviation.

We want to choose the interval in order symmetric with respect to the mean but adding a small scale "to the right", in order to not discard at all the fact that the distribution is asymmetric (Skewness $\simeq 0.55$ ).

```{r}

asymmetry.factor <- 1.20
confidence <- c()


#uniform case
for (single_value in mu){
  integral <- (pgamma(mean.analytical.u  + single_value*asymmetry.factor, alpha.post.unif, lambda.post.unif ) - pgamma(mean.analytical.u  - single_value, alpha.post.unif,  lambda.post.unif  ))
  if (integral > 0.95) {
    confidence <- c(confidence, single_value)
    break
    }
}
#mean.analytical.u
#median.analytical.u

lowerbound.u <- mean.analytical.u - confidence
upperbound.u <- mean.analytical.u + confidence*asymmetry.factor

plot( mu , post.uniform(mu), type ='l', main = "Uniform prior - CL interval" , col = 'gold3', lwd = 3)
abline (v= lowerbound.u , col = 'black' , lty=2)
abline (v= upperbound.u , col = 'black' , lty=2)
abline (v= mean.analytical.u , col = 'dodgerblue3' , lty=2)
legend("topright", inset = 0.05 , c("Posterior", 'CI', 'mean'), col = c('gold3', 'black', 'dodgerblue3') , lwd = c(3,1,1), lty = c(1,2,2))
message(sprintf("For the uniform prior we obtain a 95percent confidence interval that is [%1.2f, %1.2f] ", lowerbound.u, upperbound.u))
message(sprintf("\nFor a Normal distribution we know that the 95percent interval is centered on the mean and is fairly 2sigma large, where sigma and mu are the mean and the standard deviation found before. We can compute this CL interval easily, and it is: [%1.2f, %1.2f] ", mean.analytical.u - 2*sqrt(var.analytical.u) , mean.analytical.u + 2*sqrt(var.analytical.u)))
```


```{r}
asymmetry.factor <- 1.20
confidence <- c()


#uniform case
for (single_value in mu){
  integral <- (pgamma(mean.analytical.j  + single_value*asymmetry.factor, alpha.post.jeffrey, lambda.post.jeffrey ) - pgamma(mean.analytical.j  - single_value, alpha.post.jeffrey,  lambda.post.jeffrey  ))
  if (integral > 0.95) {
    confidence <- c(confidence, single_value)
    break
    }
}
#mean.analytical.j
#median.analytical.u

lowerbound.j <- mean.analytical.j - confidence
upperbound.j <- mean.analytical.j + confidence*asymmetry.factor

plot( mu , post.jeffrey(mu), type ='l', main = "Jeffrey's prior - CL interval" , col = 'gold3', lwd = 3)
abline (v= lowerbound.j , col = 'black' , lty=2)
abline (v= upperbound.j , col = 'black' , lty=2)
abline (v= mean.analytical.j , col = 'dodgerblue3' , lty=2)
legend("topright", inset = 0.05 , c("Posterior", 'CI', 'mean'), col = c('gold3', 'black', 'dodgerblue3') , lwd = c(3,1,1), lty = c(1,2,2))
message(sprintf("For the Jeffrey's prior we obtain a 95percent confidence interval that is [%1.2f, %1.2f] ", lowerbound.j, upperbound.j))
message(sprintf("\nFor a Normal distribution we know that the 95percent interval is centered on the mean and is fairly 2sigma large, where sigma and mu are the mean and the standard deviation found before. We can compute this CL interval easily, and it is: [%1.2f, %1.2f] ", mean.analytical.j - 2*sqrt(var.analytical.j) , mean.analytical.j + 2*sqrt(var.analytical.j)))
```
As a conclusion we can note that the normal distribution is an approximation, because it does not take into account the fact that both posteriors are asymmetric.


Exercise 2

Given the problem of the lightouse discussed last week, study the case in which both the position the shore ($\alpha$) and the distance Parameter out at sea estimation ($\beta$) are unknown.



```{r}
seed <- 1968
set.seed(seed)
true.alpha <- -1
true.beta <- 3

observations <- 250
data <- rcauchy(observations, true.alpha, true.beta) #+ rnorm(observations, 0, 0.4)
#data <- data[data > -6 & data < 6]

n <- c(2,5,10,20,35,50,100,150, 200)

post.likelihood.fun <- function(data, alpha, beta){
  if(alpha < -6 || alpha > 6 || beta < 0) { return ( 0 )}
  likelihood <- 1
  for (x in data) {
    likelihood <- likelihood*dcauchy(x, alpha, beta)
  }
  return (likelihood) 
}


n.sample <- 100
x.min <- -6; x.max <- +6
h <- (x.max - x.min )/n.sample

dist.min <- 0.1; dist.max <- 6.1
g <- (dist.max - dist.min)/n.sample

alpha <- seq(from= x.min      , by=h  , length.out = n.sample +1)
beta  <- seq(from= dist.min   , by=g  , length.out = n.sample +1)


for (samples in n){

dt <- data[1:samples]

z <- matrix (data = NA , nrow = length(alpha), ncol = length(beta))
  for(j in 1: length(alpha)) {
    for(k in 1: length(beta)) {
      z[j,k] <- post.likelihood.fun(dt, alpha[j], beta[k])
    }
  }
z.norm <- h*g*sum(z)
z <- z/z.norm

contour (alpha, beta, z, nlevels = 8,labcex = 0.5,lwd = 2, xlab=expression(alpha), ylab=expression(beta), main=sprintf("Posterior - Number of observations = %2d",samples))
abline (v=true.alpha, h=true.beta, col="grey")
ind <- which(z == max(z), arr.ind = TRUE)
alpha.max <- alpha[ind[1]]
beta.max <- beta[ind[2]]
text(x = -6, y = 5.5 , col="navy ", lwd = 2, pos=4, paste("Alpha value: ", alpha.max , sep=""))
text(x = -6, y = 5.0 , col="navy ", lwd = 2, pos=4, paste("Beta value: ", beta.max , sep=""))

text(x = +4, y = 5.5 , col="firebrick2 ", lwd = 2, pos=4, paste("Alpha true: ", true.alpha , sep=""))
text(x = +4, y = 5.0 , col="firebrick2 ", lwd = 2, pos=4, paste("Beta true: ", true.beta , sep=""))
}
```




```{r}
set.seed(seed)
observations <- 250
data <- rcauchy(observations, true.alpha, true.beta) + rnorm(observations, 0, 0.5)
#data <- data[data > -6 & data < 6]

n <- c(2,5,10,20,35,50,100,150, 200)

post.likelihood.fun <- function(data, alpha, beta){
  if(alpha < -6 || alpha > 6 || beta < 0) { return ( 0 )}
  likelihood <- 1
  for (x in data) {
    likelihood <- likelihood*dcauchy(x, alpha, beta)
  }
  return (likelihood) 
}


n.sample <- 100
x.min <- -6; x.max <- +6
h <- (x.max - x.min )/n.sample

dist.min <- 0.1; dist.max <- 6.1
g <- (dist.max - dist.min)/n.sample

alpha <- seq(from= x.min      , by=h  , length.out = n.sample +1)
beta  <- seq(from= dist.min   , by=g  , length.out = n.sample +1)


for (samples in n){

dt <- data[1:samples]

z <- matrix (data = NA , nrow = length(alpha), ncol = length(beta))
  for(j in 1: length(alpha)) {
    for(k in 1: length(beta)) {
      z[j,k] <- post.likelihood.fun(dt, alpha[j], beta[k])
    }
  }
z.norm <- h*g*sum(z)
z <- z/z.norm

contour (alpha, beta, z, nlevels = 8,labcex = 0.5,lwd = 2, xlab=expression(alpha), ylab=expression(beta), main=sprintf("w/ Noise: Posterior - Number of observations = %2d",samples))
abline (v=true.alpha, h=true.beta, col="grey")
ind <- which(z == max(z), arr.ind = TRUE)
alpha.max <- alpha[ind[1]]
beta.max <- beta[ind[2]]
text(x = -6, y = 5.5 , col="navy ", lwd = 2, pos=4, paste("Alpha value: ", alpha.max , sep=""))
text(x = -6, y = 5.0 , col="navy ", lwd = 2, pos=4, paste("Beta value: ", beta.max , sep=""))

text(x = +4, y = 5.5 , col="firebrick2 ", lwd = 2, pos=4, paste("Alpha true: ", true.alpha , sep=""))
text(x = +4, y = 5.0 , col="firebrick2 ", lwd = 2, pos=4, paste("Beta true: ", true.beta , sep=""))
}

```
It is interesting as the random noise biases the results only for fewobservations, but in the long run it does not affect our estimates of parameters alpha and beta. Moreover our confidence interval quite always (also when changing seeds) contains the true values. 


Exercise 3

Given the Signal over Background example discussed last week, analyze and discuss the following cases:

(a) Vary the sampling resolution of used to generate the data, keeping the same sampling range:
```{r}
# - Generative model
signal <- function (x, a, b, x0, w, t) {
t*(a*exp (-(x-x0)^2/(2*w^2)) + b)
}

# Define model
x0 <- 0 #Signal peak
w  <-1 #Signal width

#parameters
A.true  <- 2 # Signal amplitude
B.true  <- 1 # Background amplitude
Delta.t <- 8 # Exposure time

#Grid for evaluating the posterior
alim  <- c(0.0, 4.0)
blim  <- c(0.5, 2)
Nsamp <- 100
uniGrid <- seq(from=1/(2*Nsamp), to=1-1/(2*Nsamp), by=1/Nsamp)
delta_a <- diff(alim)/Nsamp
delta_b <- diff(blim)/Nsamp
a <- alim[1] + diff(alim)*uniGrid
b <- blim[1] + diff(blim)*uniGrid
```
Change the resolution w = {0.1, 0.25, 1, 2, 3}. 
And check the effect on the results
```{r}
# Log posterior
log.post <- function (d, x, a, b, x0, w, t) {
# prior is: both a and b must be positive
  if(a<0 || b <0) { return (-Inf )} 
  sum( dpois(d, lambda = signal (x, a, b, x0, w, t), log=TRUE ))
}
```

```{r}
SB_analysis <- function (value, A = A.true, B = B.true){
  
xdat <- seq(from=-7*value, to=7*value, by=0.5*value) 

set.seed(205)
#true signal
s.true <- signal (xdat , A , B, x0, value, Delta.t)
ddat <- rpois( length (s.true), s.true)

xplot <- seq(from=min(xdat), to=max(xdat), by=0.05*value)
splot <- signal(xplot , A , B , x0, value, Delta.t)
plot(xplot , splot , xlab="x", ylab=" Signal + Background counts ", type = 'l', lwd = 3, main = sprintf('Resolution sampling: %.2f   S/N ratio %.2f', value, A/B))
xdat.off <- xdat - value*0.5/2
lines(xdat.off , ddat , type="s",col="firebrick 3", lwd=2,xlim=range ( xplot), ylim= range (c(splot , ddat )))

# Compute log unnormalized posterior , z = ln PË†*(a,b|D), on a regular grid
z <- matrix (data = NA , nrow = length (a), ncol =length (b))
for(j in 1: length (a)) {
  for(k in 1: length (b)) {
    z[j,k] <- log.post(ddat , xdat , a[j], b[k], x0, w, Delta.t)
  }
}
z <- z - max(z) # set maximum to zero

# Plot unnormalized 2D posterior as contours .
contour (a, b, exp(z), nlevels = 5, labcex = 0.5, lwd = 2, xlab="amplitude , A", ylab="background , B", main = sprintf('Resolution sampling: %.2f   S/N ratio %.2f', value, A/B))
abline (v = A, h = B, col = "grey")

ind <- which(z == max(z), arr.ind = TRUE)
A.max <- a[ind[1]]
B.max <- b[ind[2]]

text(x = 0, y = 2 , col="navy ", lwd = 2, pos=4, paste("A value: ", A.max , sep=""))
text(x = 0, y = 1.9 , col="navy ", lwd = 2, pos=4, paste("B value: ", B.max , sep=""))

text(x = +3, y = 2 , col="firebrick2 ", lwd = 2, pos=4, paste("A true: ", A , sep=""))
text(x = +3, y = 1.9 , col="firebrick2 ", lwd = 2, pos=4, paste("B true: ", B , sep=""))

}

```


```{r}
#SB_analysis(0.1)
#SB_analysis(0.25)
SB_analysis(1)
#SB_analysis(2)
#SB_analysis(3)
```

```{r}
p_a_D <- apply(exp(z), 1, sum)
p_a_D <- p_a_D/( delta_a*sum(p_a_D))
p_b_D <- apply(exp(z), 2, sum)
p_b_D <- p_b_D/( delta_b*sum(p_b_D))
# Compute normalized conditional posteriors , P(a|b,D) and P(b|a,D)
# using true values of conditioned parameters . Vectorize (func , par)
# makes a vectorized function out of func in the parameter par.
p_a_bD <- exp( Vectorize(log.post , "a")(ddat , xdat , a, B.true ,x0, w, Delta.t))
p_a_bD <- p_a_bD/( delta_a*sum(p_a_bD))

p_b_aD <- exp( Vectorize (log.post , "b")( ddat , xdat , A.true , b,x0, w, Delta.t))
p_b_aD <- p_b_aD/( delta_b*sum(p_b_aD))

par( mfrow=c(2,2), mgp=c(2,0.8,0), mar=c(3.5,3.5,1,1), oma=0.1*c(1,1,1,1))
# Plot the 1D marginalized posteriors
plot(b, p_b_D[2:length(p_b_D)], xlab="background , B", yaxs="i", ylim=1.05*c(0,max(p_b_D, p_b_aD)), ylab="P(B | D) and P(A | B,D)", type="l", lwd=2)
lines(b, p_b_aD , lwd=2, lty=2)
abline (v=B.true , col="grey")


plot(a, p_a_D[2:length(p_a_D)], xlab="amplitude , A", yaxs="i",ylim=1.05*c(0,max(p_a_D, p_a_bD)), ylab="P(A | D) and P(A | B,D)",
type="l", lwd=2)
lines (a, p_a_bD , lwd=2, lty=2)
abline (v=A.true , col="grey")
```

```{r}
# Define function to return true signal at position x (generative model)
signal <- function(x, a, b, x0, w, t) {
  t*(a*exp(-(x-x0)^2/(2*w^2)) + b)
}

# Define function to return (natural) log posterior over (a,b).
# Prior on a and b: P(a,b) = const if a>0 and b>0, = 0 otherwise.
# Likelihood for one point is Poisson with mean d(x), so total 
# likelihood is their product. Unnormalized posterior is product of these.
# d and x are equal length vectors (or scalars). The rest are scalars.
logupost <- function(d, x, a, b, x0, w, t) {
  if(a<0 || b <0) {return(-Inf)} # the effect of the prior
  sum(dpois(d, lambda=signal(x, a, b, x0, w, t), log=TRUE))
}

# Set model parameters (true and fixed)
x0    <- 0 # centre of peak
w     <- 1 # sd of peak
atrue <- 2 # amplitude
btrue <- 1 # background
t     <- 5 # scale factor (exposure time -> sets SNR)

# Simulate some data (by drawing from the likelihood)
set.seed(205)
xdat  <- seq(from=-7*w, to=7*w, by=0.5*w)
strue <- signal(xdat, atrue, btrue, x0, w, t)
ddat  <- rpois(length(strue), strue)

# Define sampling grid to compute posterior (will be normalized
# over this range too). uniGrid spans the range 0-1 with Nsamp 
# points. This is then scaled to cover the ranges alim and blim.
alim  <- c(0.0, 4.0)
blim  <- c(0.5, 1.5)
Nsamp <- 1e2
uniGrid <- seq(from=1/(2*Nsamp), to=1-1/(2*Nsamp), by=1/Nsamp)
delta_a <- diff(alim)/Nsamp 
delta_b <- diff(blim)/Nsamp
a <- alim[1] + diff(alim)*uniGrid 
b <- blim[1] + diff(blim)*uniGrid 

# Compute log unnormalized posterior, z = ln P^*(a,b|D), on a regular grid
z <- matrix(data=NA, nrow=length(a), ncol=length(b))
for(j in 1:length(a)) {
  for(k in 1:length(b)) {
    z[j,k] <- logupost(ddat, xdat, a[j], b[k], x0, w, t)
  }
}
z <- z - max(z) # set maximum to zero

# Compute normalized marginalized posteriors, P(a|D) and P(b|D)
# by summing over other parameter. Normalize by gridding.
p_a_D <- apply(exp(z), 1, sum)
p_a_D <- p_a_D/(delta_a*sum(p_a_D))
p_b_D <- apply(exp(z), 2, sum)
p_b_D <- p_b_D/(delta_b*sum(p_b_D))

# Compute mean, standard deviation, covariance, correlation, of a and b
mean_a <- delta_a * sum(a * p_a_D)
mean_b <- delta_b * sum(b * p_b_D)
sd_a   <- sqrt( delta_a * sum((a-mean_a)^2 * p_a_D) )
sd_b   <- sqrt( delta_b * sum((b-mean_b)^2 * p_b_D) )
# To calculate the covariance I need to normalize P(a,b|D) = exp(z).
# I do it here by brute force with two loops (there are better ways in R).
# The normalization constant is Z = delta_a*delta_b*sum(exp(z)).
# This is independent of (a,b) so can be calculated outside of the loops.
# The factor delta_a*delta_b will just cancel in the expression for 
# cov_ab, so I omit it entirely.
cov_ab <- 0
for(j in 1:length(a)) {
  for(k in 1:length(b)) {
    cov_ab <- cov_ab + (a[j]-mean_a)*(b[k]-mean_b)*exp(z[j,k])
  }
}
cov_ab <- cov_ab / sum(exp(z))
rho_ab <- cov_ab / (sd_a * sd_b)
cat("  a = ", mean_a, "+/-", sd_a, "\n")
cat("  b = ", mean_b, "+/-", sd_b, "\n")
cat("rho = ", rho_ab, "\n")

# Compute normalized conditional posteriors, P(a|b,D) and P(b|a,D)
# using true values of conditioned parameters. Vectorize(func, par)
# makes a vectorized function out of func in the parameter par.
p_a_bD <- exp(Vectorize(logupost, "a")(ddat, xdat, a, btrue, x0, w, t))
p_a_bD <- p_a_bD/(delta_a*sum(p_a_bD))
p_b_aD <- exp(Vectorize(logupost, "b")(ddat, xdat, atrue, b, x0, w, t))
p_b_aD <- p_b_aD/(delta_b*sum(p_b_aD))

# Make plots
# Plot true model and data
par(mfrow=c(2,2), mgp=c(2,0.8,0), mar=c(3.5,3.5,1,1), oma=0.1*c(1,1,1,1))
xplot <- seq(from=min(xdat), to=max(xdat), by=0.05*w)
splot <- signal(xplot, atrue, btrue, x0, w, t)
plot(xplot, splot, ylim=range(c(splot, ddat)), xlab="x", ylab="s or d", 
     type="l", col="grey", lwd=2)
lines(xdat, ddat, type = 's')
# Plot unnormalized 2D posterior as contours.
# Note that they are labelled by posterior density relative to peak, 
# NOT by how much probabilty they enclose.
contour(a, b, exp(z), nlevels=5, labcex=0.5, lwd=2, xlab="amplitude, a", 
        ylab="background, b")
abline(v=2,h=1,col="grey")
# Plot the 1D marginalized posteriors
plot(b, p_b_D, xlab="background, b", yaxs="i", 
     ylim=1.05*c(0,max(p_b_D, p_b_aD)), ylab="P(b | D)  and  P(b | a,D)", 
     type="l", lwd=2)
lines(b, p_b_aD, lwd=2, lty=2)
abline(v=btrue, col="grey")
plot(a, p_a_D, xlab="amplitude, a", yaxs="i", 
     ylim=1.05*c(0,max(p_a_D, p_a_bD)), ylab="P(a | D)  and  P(a | b,D)", 
     type="l", lwd=2)
lines(a, p_a_bD, lwd=2, lty=2)
abline(v=atrue, col="grey")
```



(b) Change the ratio A/B used to simulate the data (keeping both positive in accordance with the prior).

Check the effect on the results
```{r}
SB_analysis(1, A = 2, B = 0.6)
SB_analysis(1, A = 2, B = 0.8)
SB_analysis(1, A = 2, B = 1.2)
SB_analysis(1, A = 1, B = 1)
SB_analysis(1, A = 1, B = 1.5)
```
